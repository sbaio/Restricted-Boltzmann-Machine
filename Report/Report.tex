% \documentclass[a4paper,10pt]{article}

% \usepackage[utf8]{inputenc}
% \usepackage{natbib}
% %\usepackage[vmargin=2cm]{geometry}
% \usepackage{fullpage}

% \setlength{\bibsep}{1pt}
% \usepackage{graphicx}
% \graphicspath{{Figures/}}

% \usepackage{hyperref}
% \hypersetup{
%   colorlinks, linkcolor=red
% }

% \renewcommand{\bibfont}{\small}


% %\title{Project report - Object Recognition and Computer vision}
% \title{%
% 	\huge{Training Restricted Boltzmann Machines}\\ \bigbreak
%   	\Large{Project report}\\ 
%   	\Large{Probabilistic Graphical Models - MVA}
% }
% \author{Chaimaa Kadaoui, Othman Sbai, Xi Shen}
% \date\today

\documentclass{article}
\PassOptionsToPackage{numbers, compress}{natbib}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[]{amsmath}
\usepackage[]{graphicx}
\usepackage[]{float}
\graphicspath{{Figures/}}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}


\DeclareMathOperator*{\argmax}{\arg\!\max}
\title{Restricted Boltzmann Machines}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Chaimaa Kadaoui  \\
  \texttt{chaimaa.kadaoui\thanks{mail extension @eleves.enpc.fr for all authors}} \\
  \And
  Othman SBAI \\
  \texttt{othman.sbai} \\
  \And
   Xi Shen\\
  \texttt{xi.shenxi} \\
}


\begin{document}

\maketitle
\section{Abstract}
For our project in Probabilistic Graphical Models course, we studied the use and the training of Restricted Boltzmann Machines as an instance of undirected graphical models. We mainly base our approach on the practical guide for training Restricted Boltzmann Machines (Hinton 2010~\cite{hinton2010practical}). We start by an introduction to RBM and the interest that they have raised in the last years, then we define the theoretical aspects of main training methods of RBM. Finally we present our results by comparing our implementation of training RBM to an implementation from another deep learning library, on two popular image datasets MNIST and CIFAR-10. We show our convergence results and investigate the influence of different parameters of the training algorithm.

% Here introduction about the place that occupy restricted boltzmann machines currently among deep learning algorithms. Are they used? by whom ? which latest posts and papers? for which interest ? 

% Describe the evolution of the interest in RBM during last decades. 
% How advances in training techniques lead to better use of RBM, like CD ?

% why we chose to work on this topic

% Describe the approach of the topic : training RBM
\section{Introduction}
\subsection{Definition of an RBM}

A Restricted Boltzmann Machine (RBM) is an undirected energy-based probabilistic graphical model which can be seen as a two layer neural network (visible and hidden units). RBMs can be used to model and learn important aspects of a probability distribution of a training data. They are called restricted, because we impose restrictions on the network topology, by allowing only connections between units of different layers. RBM are energy-based, since they define probability distribution through an energy function. Learning corresponds to shaping the energy so that desirable configurations have a low energy and thus maximize probability of training data under the model. Maximum likelihood learning is challenging for undirected graphical models because MLE parameters cannot be found analytically and the log likelihood gradient based optimization is not tractable. This optimization requires obtaining samples through Markov Chain Monte Carlo, which is computationally demanding.

% what about EM algorithm ?

\subsection{Interest in RBM}

A major interest have been dedicated to directed graphical models with one layer of observed variables and one or more layers of hidden variables such Mixture of Gaussians, probabilistic PCA, factor analysis, latent dirichlet analysis LDA... However, the undirected two layered analogue which was first introduced and called Harmonium by \cite{smolensky1986information} and renamed as "Restricted Boltzmann Machines" by \cite{hinton2002training} have also seen advances through new training methods, which unlike directed graphical models, makes inference and learning very fast. %This last paper from Hinton in 2002, introduces a concept of Product of Experts which multiplies probability distributions of many experts. An RBM is then a PoE with one expert per hidden units

%https://papers.nips.cc/paper/2672-exponential-family-harmoniums-with-an-application-to-information-retrieval.pdf
%http://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf

\section{Applications of RBM}
% to be reformulated /.. copied ../
Restricted Boltzmann machines have successfully been applied to problems involving high dimensional data such as images, text  \cite{hinton2006fast}, \cite{vincent2010stacked} as unsupervised feature extractors.

Recent developments have demonstrated the capacity of RBM to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In this setting they can be seen as good preprocessing or initialization for some other models. They can also be used as self-contained classifiers as shown in \cite{larochelle2012learning}.

\begin{itemize}
		\item \textbf{RBM as a generative model}: learns and generates samples from data distribution 
		\item \textbf{RBM as feature extractor}: unsupervised feature extraction from data, instead of handcrafting features 
		\item \textbf{RBM for computer vision}: such as object recognition, image denoising and inpainting.
		\item \textbf{RBM for collaborative filtering}: given a set of N users and M movies, we would like to recommend movies to the users.
	\end{itemize}

\section{Theory of RBM}

\subsection{Definition of RBM}
Restricted Boltzmann Machines are particular instance of undirected graphical models inspired by neural networks, where the units are organized in two layers.

\begin{itemize}
    \item The visible layer $ \mathbf{x} $ which contains all the visible nodes.
    \item The hidden layer $ \mathbf{h} $ with all the latent variables.
\end{itemize}

It is \emph{restricted} because no connections between nodes of the same layer are allowed. Each node of one layer is connected to all the other nodes of the other layer ($h_j \leftrightarrow x_k$). The matrix $W$ characterizes the connections between the two layers: $ W_{j,k} $ is the value for the connection between $h_j$ and $x_k$. For simplification, we suppose that the variables $ \mathbf{x} $ and $ \mathbf{h} $ are binary.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{rbm.png}
\label{fig:RBM}
\caption{The network graph of an RBM}
\end{figure}

\subsection{Energy and probability}
\label{subsec:energy}
We introduce the two bias vectors $c$ and $b$ to define the energy of this graph:

\[ E(x,h) = -h^\top W x - c^\top x - b^\top h \]

We can write the joint probability of $ \mathbf{x} $ and $ \mathbf{h} $ as: 

\[ p(x,h) = \frac{1}{Z} \exp(-E(x,h)) \]

$Z$ is the partition parameter which can be computed by calculating all the possibles values of $ \mathbf{x} $  and $ \mathbf{h} $. In practice, this parameter is intractable.

We see that to increase the probability, we want to decrease the energy. We have:
 
\begin{itemize}
    \item if $c_k < 0$ and $x_k = 1$, the energy is higher; therefore we prefer having $x_k = 0$ (the probability of $x_k = 1$ decreases).
    \item if $c_k > 0$, similarly, we prefer $x_k = 1$ over $x_k = 0$.
\end{itemize}

We apply the same arguments for $b$ and $W$ and deduce similar interpretations of theses parameters.

\section{Inference: Conditional inference}

Because $p(x)$ is intractable, the only possible type of inference is computing conditional inference. We will prove that computing the conditional probabilities $p(x|h)$ and $p(h|x)$ is easy.

Because of the construction of the graph, the variables $h_1, \hdots, h_H$ are independant conditionally on $\mathbf{x}$: \[p(h|x) = \prod_j p(h_j|x)\]

We can also prove that given $\mathbf{x}$, $h_j$ follows a Bernoulli:

\[ p(h_j=1|x) = sigm(b_j + W_{j.}x)\]

$ W_{j.}$ is the j-th row of $W$ and $sigm$ defines the sigmoid function: $$sigm(x) = \frac{1}{1+e^{-x}}$$

We can prove that either by derivation of  $p(h|x)$ or by applying the \emph{Local Markov Property}: $p(z_i|z_j, \; j \neq i) =  p(z_i|\mathcal{N}(z_i))$.

\subsection{Free energy}

To have a better understanding of the parametrs $W \;, c \;, b$, let's introduce the concept of free energy. Using a marginalization and some derivations, we can write the probability $p(x)$ as :

\[ p(x) = \sum_h p(x,h) = \exp\left(c^\top x + \sum_{j=1}^H \log\left( 1+\exp(b_j + W_{j.}x) \right) \right) / Z \]

The function $u \mapsto \log\left( 1+\exp(u) \right)$ is called \texttt{softplus} and is a soft approximation of $u \mapsto \max(u,0)$. \\

We can see that $p(x) = \exp\left(-F(x)\right) / Z$ where $F$ is the free energy. We want to increase the probability of our data $x$ and thus decreases its energy. Let's look at each term of this energy:

\begin{itemize}
    \item As seen in the previous subsection (\ref{subsec:energy}), the sign of $c_k$ impacts wether $x_k$ would be a 0 or 1. Thus, having $c^\top x$ in the probability means that we want $\mathbf{x}$ and the bias $c$ to be aligned. This bias characterizes the probability.
    \item Similarly, $\mathbf{x}$ and $W_{j.}$ should align for each $j$. And we can see $b_j$ as a control parameter: $W_{j.}x$ has to be big enough to have $b_j + W_{j.}x > 0 $.  Therefors, we can considers the hidden variables as features and $b_j$ the bias for each feature (it defines the importance of each feature). The goal of the RBM is to represent meaningful features.
\end{itemize}

 

\subsection{Contrastive divergence theory ...}



\clearpage
\section{Implementation and results}
\subsection{Implementation of RBM on python}

\subsection{Implementation of RBM on tensorflow using a GPU}

We use the implementation in tensorflow provided by Gabriele Angeletti as a library gathering many deep learning models and algorithms such as convolutional networks, RNN, RBM, Deep Belief Networks...


% define the loss we are using
% compare the loss evolution over epochs while varying parameters 
% reconstruct hidden layers?

\clearpage
\nocite{*}
\bibliographystyle{plain}
\bibliography{biblio}


\end{document}



% overfitting ?
% convergence ?